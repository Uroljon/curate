#!/usr/bin/env python3
"""Comprehensive test for the full extraction pipeline."""

import sys
import os
import time
import json
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from semantic_chunker import smart_chunk, is_heading
from embedder import embed_chunks, query_chunks, model
from semantic_llm_chunker import prepare_semantic_llm_chunks, analyze_chunk_quality


def test_pipeline_components():
    """Test each component of the pipeline with sample German text."""
    
    print("ğŸ§ª CURATE Pipeline Component Tests\n")
    print("="*60)
    
    # Sample German municipal document text
    sample_text = """NACHHALTIGKEITSSTRATEGIE DER STADT

Kapitel 1: EinfÃ¼hrung

Die Stadt hat sich zum Ziel gesetzt, bis 2030 klimaneutral zu werden. Dies erfordert umfassende MaÃŸnahmen in allen Bereichen.

Handlungsfeld: Klimaschutz

Die Reduzierung der CO2-Emissionen um 55% bis 2030 ist das zentrale Ziel. Dabei spielen folgende Projekte eine wichtige Rolle:

Projekte:
- Ausbau erneuerbarer Energien auf 100% bis 2035
- Energetische Sanierung mit 3% Sanierungsquote pro Jahr
- FÃ¶rderung der ElektromobilitÃ¤t mit 500 neuen Ladepunkten

Indikatoren:
- CO2-Reduktion: 55% bis 2030
- Erneuerbare Energien: 100% bis 2035
- Sanierungsquote: 3% pro Jahr
- Ladepunkte: 500 bis 2025

Handlungsfeld: MobilitÃ¤t

Die Verkehrswende ist ein zentraler Baustein fÃ¼r eine nachhaltige Stadt.

MaÃŸnahmen:
- Ausbau des Radwegenetzes auf 500 km
- EinfÃ¼hrung einer Stadtbahn mit 18 km StreckenlÃ¤nge
- Modal Split von 70% fÃ¼r den Umweltverbund

Zielstellung:
30% weniger PKW-Verkehr bis 2030."""

    all_tests_passed = True
    
    # Test 1: German-aware chunking
    print("\n1ï¸âƒ£ Testing German-aware chunking...")
    try:
        chunks = smart_chunk(sample_text, max_chars=5000)
        
        # Check if headings are detected
        heading_count = 0
        for chunk in chunks:
            lines = chunk.split('\n')
            for line in lines:
                if is_heading(line.strip()):
                    heading_count += 1
        
        print(f"   âœ… Created {len(chunks)} chunks")
        print(f"   âœ… Detected {heading_count} headings")
        
        # Verify key sections are preserved
        full_text = '\n\n'.join(chunks)
        assert "Handlungsfeld: Klimaschutz" in full_text
        assert "Handlungsfeld: MobilitÃ¤t" in full_text
        assert "55% bis 2030" in full_text
        print("   âœ… All key sections preserved")
        
    except Exception as e:
        print(f"   âŒ Failed: {e}")
        all_tests_passed = False
    
    # Test 2: Multilingual embeddings
    print("\n2ï¸âƒ£ Testing multilingual embeddings...")
    try:
        # Test German-German similarity
        german_texts = ["Klimaschutz und Energie", "Umweltschutz und Energiewende"]
        embeddings = model.encode(german_texts)
        
        from numpy import dot
        from numpy.linalg import norm
        
        similarity = dot(embeddings[0], embeddings[1])/(norm(embeddings[0])*norm(embeddings[1]))
        print(f"   âœ… German-German similarity: {similarity:.3f}")
        assert similarity > 0.7, "German similarity should be high"
        
        # Test German-English cross-lingual
        mixed_texts = ["Klimaschutz", "climate protection"]
        mixed_embeddings = model.encode(mixed_texts)
        cross_similarity = dot(mixed_embeddings[0], mixed_embeddings[1])/(norm(mixed_embeddings[0])*norm(mixed_embeddings[1]))
        print(f"   âœ… German-English similarity: {cross_similarity:.3f}")
        assert cross_similarity > 0.6, "Cross-lingual similarity should work"
        
    except Exception as e:
        print(f"   âŒ Failed: {e}")
        all_tests_passed = False
    
    # Test 3: Semantic LLM chunking
    print("\n3ï¸âƒ£ Testing semantic LLM chunking...")
    try:
        # Create test chunks
        test_chunks = [
            "Handlungsfeld: Klimaschutz\n\nWichtige MaÃŸnahmen fÃ¼r den Klimaschutz.",
            "Weitere Details zum Klimaschutz mit Indikatoren.",
            "Handlungsfeld: MobilitÃ¤t\n\nDie Verkehrswende ist wichtig.",
            "ZusÃ¤tzliche MobilitÃ¤tsmaÃŸnahmen und Projekte."
        ]
        
        # Test semantic chunking
        semantic_chunks = prepare_semantic_llm_chunks(test_chunks, max_chars=10000, min_chars=5000)
        
        print(f"   âœ… Input chunks: {len(test_chunks)}")
        print(f"   âœ… Output chunks: {len(semantic_chunks)}")
        
        # Check that Handlungsfelder are not mixed
        for i, chunk in enumerate(semantic_chunks):
            klimaschutz_count = chunk.count("Klimaschutz")
            mobilitÃ¤t_count = chunk.count("MobilitÃ¤t")
            
            if klimaschutz_count > 0 and mobilitÃ¤t_count > 0:
                print(f"   âš ï¸  Chunk {i+1} mixes Handlungsfelder!")
            else:
                print(f"   âœ… Chunk {i+1} maintains section integrity")
        
        # Analyze quality
        quality = analyze_chunk_quality(semantic_chunks)
        print(f"   âœ… Chunks with headings: {quality['heading_stats']['chunks_with_headings']}/{quality['total_chunks']}")
        
    except Exception as e:
        print(f"   âŒ Failed: {e}")
        all_tests_passed = False
    
    # Test 4: Indicator detection
    print("\n4ï¸âƒ£ Testing indicator patterns...")
    try:
        indicator_patterns = [
            ("55% bis 2030", True, "percentage with year"),
            ("500 neue Ladepunkte", True, "number with unit"),
            ("3% pro Jahr", True, "percentage per year"),
            ("18 km StreckenlÃ¤nge", True, "distance measurement"),
            ("Modal Split 70%", True, "modal split percentage"),
            ("normale text", False, "regular text")
        ]
        
        import re
        patterns = [
            r'\d+\s*%',  # Percentages
            r'bis\s+20\d{2}',  # Year targets
            r'\d+\s*(km|mÂ²|MW|ha|t)',  # Units
            r'\d+\s+(neue|mehr|weniger)',  # Quantities
        ]
        
        for text, should_match, description in indicator_patterns:
            matched = any(re.search(pattern, text) for pattern in patterns)
            if matched == should_match:
                print(f"   âœ… '{text}' - {description}")
            else:
                print(f"   âŒ '{text}' - detection failed")
                all_tests_passed = False
                
    except Exception as e:
        print(f"   âŒ Failed: {e}")
        all_tests_passed = False
    
    # Summary
    print("\n" + "="*60)
    if all_tests_passed:
        print("âœ… All pipeline components working correctly!")
        print("\nThe pipeline is ready for:")
        print("  â€¢ German document processing")
        print("  â€¢ Semantic chunk preservation") 
        print("  â€¢ Multilingual understanding")
        print("  â€¢ Indicator extraction")
    else:
        print("âŒ Some tests failed. Please check the implementation.")
    
    return all_tests_passed


def test_embedding_storage():
    """Test that embeddings can be stored and retrieved correctly."""
    print("\n\n5ï¸âƒ£ Testing embedding storage and retrieval...")
    
    test_source_id = "test_pipeline_doc"
    test_chunks = [
        "Handlungsfeld: Testbereich\n\nDies ist ein Testchunk fÃ¼r die Pipeline.",
        "Indikatoren: 50% Reduktion bis 2030, 100 neue Einheiten"
    ]
    
    try:
        # Store embeddings
        embed_chunks(test_chunks, source_id=test_source_id)
        print(f"   âœ… Stored {len(test_chunks)} chunks")
        
        # Retrieve chunks
        retrieved = query_chunks("Testbereich", top_k=10, source_id=test_source_id)
        print(f"   âœ… Retrieved {len(retrieved)} chunks")
        
        # Verify content
        retrieved_texts = [r["text"] for r in retrieved]
        assert any("Testbereich" in text for text in retrieved_texts), "Should retrieve test content"
        print("   âœ… Content verified")
        
        # Clean up test data
        from embedder import collection
        existing = collection.get(where={"source": test_source_id})
        if existing and existing["ids"]:
            collection.delete(ids=existing["ids"])
            print("   âœ… Cleaned up test data")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Failed: {e}")
        return False


if __name__ == "__main__":
    print("ğŸš€ CURATE Full Pipeline Test Suite")
    print("  Testing German PDF extraction pipeline components")
    
    # Run component tests
    components_ok = test_pipeline_components()
    
    # Run storage test
    storage_ok = test_embedding_storage()
    
    # Overall result
    print("\n" + "="*60)
    print("ğŸ“Š FINAL RESULT:")
    if components_ok and storage_ok:
        print("âœ… All tests passed! Pipeline is working correctly.")
        print("\nNext steps:")
        print("1. Upload a PDF via /upload endpoint")
        print("2. Extract with /extract_structure_fast")
        print("3. Enjoy better German document understanding! ğŸ‡©ğŸ‡ª")
    else:
        print("âŒ Some tests failed. Please fix issues before using pipeline.")
    
    sys.exit(0 if (components_ok and storage_ok) else 1)